# -*- coding: utf-8 -*-
"""Analysing Classification algorithms.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rIlebVabJLRfcKXec5xP3h6cdDSgLz4z

#Implementing and analysing different machine learning algorithms to check for the behaviour of these algorithms on the dataset.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import sklearn
import IPython.display as ipd
from IPython.display import Image
import warnings
warnings.filterwarnings('ignore')
import io

"""# Dataset Exploration
Music conoeisseurs have been trying for a long time to understand sound and what differenciates one song from another. How to visualize sound? What makes a tone different from another?

In the following lines we will go through an in depth analysis of sound and how we can classify and ultimately understand it.


"""

data = pd.read_csv('https://raw.githubusercontent.com/dwipddalal/Analysing_Mulitple_different_ML_algo/main/Data_of_Music.csv')
data.head()

data.describe()

data.shape

"""##visualization of this dataset:

1. #### Correlation heatmap for all the mean variables [Columns based on Mean]:
"""

data.columns
cols = []
for col in data.columns:
  if 'mean' in col:
    cols.append(col)
corr = data[cols].corr() #corr() is used to find the pairwise correlation of all columns in the dataframe. Any none values are automatically excluded.
print(corr) # is correation matrix
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)
plt.show()
g = corr.abs()
print(g)

def value_seperator(a):
  if 0.6<a and a<=1:
    b = '2:lot of correlation'
  elif 0.2<a and a<=0.6:
    b = '1:moderate correlation'
  elif -0.2<a and a<=0.2:
    b = '0:no correlation'
  elif -0.6<a and a<=-0.2:
    b = '-1:moderately_neg'
  else:
    b = '-2:opp. correlation'
  return b

corr.applymap(lambda x: value_seperator(x))

"""#Lets change this (N,M) matrix to a more readable (N*M, 1) matrix. I have also sorted the new matrix formed."""

# first we have to convert the matrix to one-dimensional series. The unstack() function is used to do so. The series will have multiple index.
# For sorting sort_values() function is used. The sort_values() function sorts a data frame in Ascending or Descending order of passed Column. 
# Retain upper triangular values of correlation matrix and
# make Lower triangular values Null
upper_corr_mat = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool)) # k = 1 for upper triu
unique_corr_pairs = upper_corr_mat.unstack().dropna() # to drop null values which is essentially lower matrix
sorted_mat = unique_corr_pairs.sort_values()
print(type(sorted_mat)) #One-dimensional ndarray with axis labels
#print(sorted_mat[0])
N_Mcolumn = sorted_mat.to_frame()

sorted_mat

"""2. #### Countplot for the labels column"""

sns.countplot(x = 'label', data = data)

"""### Boxplot for the tempo vs labels column to see number of outliers it may have"""

#data[["label", "tempo"]]
sns.boxplot(y = 'tempo', x= 'label', data = data)
#data.columns

j = data['zero_crossing_rate_var']
print(type(j))
mean = np.mean(j)
sd= np.std(j)
k = 0 
for i in j:
  z = (i-mean)/sd
  if np.abs(z)>3:
    k = k+1
print('zero_crossing_rate_var=',k)

j = data['harmony_mean']
mean = np.mean(j)
sd= np.std(j)
k = 0 
for i in j:
  z = (i-mean)/sd 
  if np.abs(z)>3:
    k = k+1
print('harmony_mean=',k)

j = data['rms_var']
mean = np.mean(j)
sd= np.std(j)
k = 0 
for i in j:
  z = (i-mean)/sd
  if np.abs(z)>3:
    k = k+1
print('rms_var=',k)

j = data['perceptr_mean']
mean = np.mean(j)
sd= np.std(j)
k = 0 
for i in j:
  z = (i-mean)/sd
  if np.abs(z)>3:
    k = k+1
print('perceptr_mean=',k)

"""### Now let's perform Machine Learning Classification. We'll be predicting the genre based on the given features."""

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier, XGBRFClassifier
from xgboost import plot_tree, plot_importance
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFE

data.head()

"""### Preprocess the data.

"""

Q1 = data.quantile(0.25)
print(Q1)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1 # interquatile range.
data = data[~((data < (Q1 - 1.5 * IQR)) |(data > (Q3 + 1.5 * IQR))).any(axis=1)]
print("SHAPE:",data.shape)  
y = data['label'] # genre variable.
X = data.loc[:, data.columns != 'label'] 
cols = X.columns # contains name of all the columns 
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)
X = pd.DataFrame(np_scaled, columns = cols)

X_train, X_test, y_train, y_test = train_test_split(X, y,stratify = y,test_size=0.1, random_state=42)

"""### The Predefined function to assess the accuracy of a model."""

def score(model, title = "Default"):
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    print(confusion_matrix(y_test, preds))
    accuracy = round(accuracy_score(y_test, preds), 5)
    print('Accuracy for', title, ':', accuracy, '\n')

"""### Let's see how a basic Logistic Regressor works on this!"""

lg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')
score(lg, "Logistic Regression")

"""The accuracy hovers around 74%.

### Next I shall run the **AdaBoostClassifier** with n_estimators=1000 & random_state=0 and **RandomForestClassifier** with n_estimators=1000, max_depth=10, random_state=0
"""

# AdaBoostClassifier
ada_clf = AdaBoostClassifier(n_estimators=300, random_state=0)
score(ada_clf, title = 'Ada' )

# Random Forest
rn = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0)
score(rn, title = 'Rn' )

"""AdaBoost performs poorly, but Random Forest looks great!"""

# Naive Bayes: GaussianNB
NBG = GaussianNB()
score(NBG, 'NBG')


KNN = KNeighborsClassifier(n_neighbors=100)
score(KNN,'KNN')

# Decission trees: DecisionTreeClassifier
dt = DecisionTreeClassifier()
score(dt,'dt')

"""## Next let's try **XGBClassifier** with n_estimators=1000 and learning_rate=0.05"""

# XGBClassifier
xg = XGBClassifier(n_estimators=1000, learning_rate=0.05, random_state = 0)
score(xg,'xg')

#classification matrix
xg = XGBClassifier(n_estimators=1000, learning_rate=0.05)
xg.fit(X_train, y_train)
preds = xg.predict(X_test)
print(sklearn.metrics.classification_report(y_test,preds))

"""**XGBRFClassifier** on your own."""

# XGBRFClassifier
xgbrf = XGBRFClassifier(n_estimators=1000, learning_rate=0.05)
xgbrf.fit(X_train, y_train)
preds = xgbrf.predict(X_test)
print(sklearn.metrics.classification_report(y_test,preds))

score(xgbrf,'xgbrf')

"""### Till now, it looks like XGBClassifier performs the best. Let's experiment with it a bit more.

Plot of the Confusion Matrix for XGBClassifier with parameters n_estimators=1000, learning_rate=0.05
"""

# Cell to plot Confusion Matrix
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(xg, X_test, y_test)  
plt.show()

"""Since featre importance is an important aspect as it tells us about which featrue contributes more to the final outcome so let's implement it. 

"""

k = xg.feature_importances_
j = k.max()
a = -1
# outputs are in percentage
for i in data.columns[:-1]:
  a = a + 1
  print(i," -> ",k[a]*100)
  if k[a] == j:
    I = i
    J = j

print("The feature with max importance is:",I, " and it's value is: " ,J*100)

